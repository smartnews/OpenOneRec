Q: I observed the item_understand dataset and noticed that the video descriptions aren't simply about the video titles and ASR recognition data; they also include descriptions of video covers and camera movements. I'm very curious about your specific process for generating text descriptions for videos, so I can generate similar descriptions for my own dataset.

A:
**Evaluation Dataset (`benchmark_data/item_understand`)** Designed solely for evaluation, this dataset contains high-quality captions generated by `gemini-2.5-pro`. The model was provided with the video title and 5 frame, using the prompt below:

> 你是一名专业的视频内容描述专家。请基于我提供的视频的标题以及视频中截取的图片，输出一段及其详尽的Caption，要求仔细分析我提供的图片，使用自然语言详细描述视频的主要内容。输出内容只包括对视频的描述，不要包括任何其他内容。这段视频的标题为: {title}。这段视频的抽帧依次为:{img_1,img2,...,img5}

**Training Dataset (`pid2caption.parquet`)** This dataset is used for training. Its captions were generated using the [VideoChat2](https://arxiv.org/pdf/2311.17005.pdf) model.

Q: 按照quick start，预训练的数据处理需要跑split_data.py脚本，该脚本将合并两份数据，OpenOneRec/OpenOneRec-General-Pretrain和OpenOneRec/OpenOneRec-RecIF，但是这两份数据的columns完全不一致，似乎不应该进行合并

A:
首先感谢对我们工作的关注！

根据你的描述，你遇到的问题可能是**脚本中`REC_DATA_PATH`这个变量配置成OpenOneRec-RecIF原始路径**导致的：

* **OpenOneRec-General-Pretrain** 这份数据混合了多份通用文本数据集，为了方便实验，我们直接提供了处理之后的训练格式。
* **OpenOneRec-RecIF** 这份数据是用户的原始行为数据，为了便于大家基于这份数据进行进一步调研和迭代，因此我们提供了一张用户行为原始表和对应的处理脚本。

这里的处理链路可能比较长，预期的处理流程是：

1. 处理RecIF原始表，生成pretrain格式的数据（[README: Process Recommendation Data](https://github.com/Kuaishou-OneRec/OpenOneRec/tree/main/data#2-process-recommendation-data)）
2. 执行脚本将General与RecIF数据合并，这里**`REC_DATA_PATH`需要设置成上一步的输出**（[README: Pretraining Data Sharding](https://github.com/Kuaishou-OneRec/OpenOneRec/tree/main/data#3-pretraining-data-sharding)

具体可以参考：[data/README.md](https://github.com/Kuaishou-OneRec/OpenOneRec/blob/main/data/README.md)

Q: During online inference, does the model output include the tag? I would like to confirm if the model generates the reasoning/thinking traces (wrapped in tags) by default when serving the request, or if this is stripped out?

A:
Thanks for your interest in our work. And, yes, the output of the model include tag by default.

If you want to close thinking mode, there are two different ways:

1. **Soft switch:** append the `/no_think` sequences to the end of your user prompt.
2. **Hard switch:** set `enable_thinking=false` when calling `apply_chat_template` (for transformers-like libs), or add an empty thinking trace ("<think>\n</think>\n\n") directly at the begin of assistant response.

You can find more detailed information from [Qwen3: switching-between-thinking-and-non-thinking-mode](https://huggingface.co/Qwen/Qwen3-0.6B#switching-between-thinking-and-non-thinking-mode).

Q: I’m trying to finetune the released OneRec foundation models on a new dataset (new catalog/items), and I’m confused about how to reproduce the “itemic tokens / semantic IDs” pipeline in practice.

A:
To help clarify the pipeline for generating "itemic tokens / semantic IDs" (SIDs) for new datasets, we have just updated the repository with a new tokenizer folder containing the necessary scripts and instructions.

Here is the recommended workflow to compute the 3-layer SIDs for a new domain:

1.Generate Embeddings: To utilize our foundation model effectively, you must first generate embeddings for your new items. It is required to use the Qwen3-8B-Embedding model (https://huggingface.co/Qwen/Qwen3-Embedding-8B) to encode your item metadata (e.g., title, description) into vectors.

2.Get Tokenizer Weights: We have released the pre-trained tokenizer weights. Please download them from Hugging Face here: OpenOneRec/OneRec-tokenizer (https://huggingface.co/OpenOneRec/OneRec-tokenizer).

3.Run Inference (Quantization): Once you have the item embeddings and the pre-trained weights, you can use the script infer_res_kmeans.py provided in the tokenizer folder to generate the discrete codes (SIDs). You do not necessarily need to retrain the tokenizer if you want to align with the foundation model; simply map your new embeddings to the existing codebook using this script.

Please check the README.md in the new tokenizer directory for full installation and usage details.

Q: Could you clarify / provide guidance on:
1. Finetuning on a new dataset
   •	Recommended training stages when adapting to a new catalog:
   •	Do we need a Stage-1 “itemic-text alignment” step to learn embeddings for new SID tokens?
   •	Or is the recommended approach to reuse the existing codebook and only finetune on interaction sequences?
   •	Any example scripts or minimal recipe for: data formatting, sequence construction, and objective (next-item generation / instruction tasks)?
2. Handling SID collisions in narrow domains
   •	In Section 6.3.1, collisions are a major issue (>30%). For Strategy 3 (itemic tokens + keywords):
   •	What is the exact input serialization you recommend (e.g., [sid_tokens] + [5 keywords] format)?

A: Regarding your remaining questions about finetuning strategies and data formatting:

1.Finetuning & Alignment: When adapting to a new domain, we recommend incorporating alignment tasks to help the model learn the representations of the new SID tokens, similar to the approach used in **LC-Rec**. You can also employ a Stage-1 "itemic-text alignment" step to re-align the features (following the OneRec-Think approach), but this is not strictly mandatory.

2.Data Formatting & Collision Handling: We are planning to open-source the relevant code for data formatting, sequence construction, and collision handling (including the specific input serialization for Strategy 3) in around 3 days. Please stay tuned for upcoming updates, which will provide the concrete steps and recipes you need.

Q: how about Multimodal data?
A: In our current pipeline, the tokenizer is trained exclusively on text-based semantic embeddings. As a result, to directly use the released OneRec foundation model and tokenizer, we recommend following the same setup and computing SIDs from text embeddings, ensuring alignment with the pre-trained codebook and model representations. While multimodal embeddings are also provided in the dataset to support future research, they are not used in training the released tokenizer. Incorporating additional modalities at the tokenizer level would therefore require retraining or redesigning the tokenizer to properly handle multimodal embeddings.

Q: I'd like to know whether the main results in Table 7 are zero-shot results, which means no any additional training, and just generate semantic IDs on the new datasets, then the pre-trained model achieved the performance reported in the main results.
A: Thank you for your question. We would like to clarify the experimental setting for the results presented in Table 7. The main results reported in Table 7 were obtained by fully fine-tuning the pre-trained OneRec-Foundation model on the target Amazon datasets.
Q: So "fully fine-tuning the pre-trained OneRec-Foundation model on the target Amazon datasets" means "Full-data training with joint multi-domain strategy"? Did you re-perform "pre-training" and "post-training" on the Amazon dataset? Could you share more details about the fine-tuning process?
A: Yes, "fully fine-tuning the pre-trained OneRec-Foundation model on the target Amazon datasets" refers to the "Full-data training with joint multi-domain strategy."
The code for the fine-tuning process on the Amazon dataset will be open-sourced shortly. Please stay tuned!

Q: I take a look a the implementation of RQ-KMeans in the repo and it seems that the it is limiting the max number of samples used here. From the implementation, it seems that the trainer would directly read whole training samples and run the RQ-KMeans. I'm curious if this is the production setting or this is just for demo purpose. Based on my understanding, we need to train RQ-KMeans on all item embeddings?
A: Regarding the sampling limit, you are correct. In our production settings, we limit the number of training samples for efficiency. We found that training the RQ-KMeans model on a subset of item embeddings is sufficient to achieve effective performance, especially when the total number of items is massive.
However, for your use case, if your item set is not extremely large, I would recommend using all item embeddings for training to ensure the best possible coverage.

Q: May I confirm whether the stage-1 warm-up pretraining is initialized from Qwen3-1.7B or Qwen3-1.7B-Base? Do you have any experimental results comparing the two?
A: Our model is initialized from Qwen3-1.7B, because this work aims to focus more on recommendation tasks, and transfer instruct and thinking capabilities from general task at a relatively lower cost. So we directly adopt Qwen-1.7B as our base model.

Q: Quick question on transfer learning strategies to decrease item SID collision rate:
1. In Strategy 2 (Text-Only) and Strategy 3 (Text-Augmented Itemic Tokens), must keywords have the same token length? If not, could longer token sequences bias generation probability?
2. How are keywords selected? In Strategy 3, how many keywords are used per item?
Also — will the code for these adaptive strategies be released?
A:
For question1, we don't limit the token length for keywords. As for your concern:

> could longer token sequences bias generation probability?

I don't think so, you could check the `model.generate()` or the paper - [D3](https://arxiv.org/abs/2406.14900) for more details.

For question2, please check our colleagues' work - [GRLM](https://arxiv.org/html/2601.06798v1).

Q: pretrain一阶段的数据是recif-pretrain中的全部数据，二阶段是general-pretrain和recif-pretrain中的全部数据，对吗
A: 在pretrain的stage1阶段我们也混入了general task的纯文本数据，主要目的是为了让itemic token的训练更稳定。
在stage1，尽管只训练itemic token且纯文本样本不包含itemic token，但文本token产生的梯度也会通过softmax回传到itemic embedding和lm_head上，为了让训练更稳定，我们在对齐阶段也混了一些通用文本数据。
在open系列模型中，由于总数据量比较有限，我们直接对stage2数据采样了一部分作为stage1的数据（包含通用文本），可以理解成stage1数据是stage2的子集，且两阶段数据分布基本一致。pro系列模型中，我们有单独针对两个阶段的数据调整数据配比和数据集的筛选。Tech report中的Appendix B.4. 简单列了在不同阶段通用文本与reco数据的配比，可以作为参考。